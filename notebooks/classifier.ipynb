{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "super_directory = os.path.abspath('..')\n",
    "sys.path.append(super_directory)\n",
    "\n",
    "from data_setup import get_dataloaders\n",
    "from vit import DataEmbeddings, EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Patches\n",
    "PATCH_SIZE = (7, 7)\n",
    "NUM_PATCHES = int((28 / 7) ** 2)\n",
    "\n",
    "# Patches to Embeddings\n",
    "EMBED_DIMS = 48\n",
    "\n",
    "# Number of Attention heads\n",
    "NUM_ATTENTION_HEADS = 12\n",
    "\n",
    "# Number of hidden layers in MLP block\n",
    "RATIO_HIDDEN_MLP = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_path = Path('../data/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dataloaders\n",
    "train_dataloader, test_dataloader, class_labels = get_dataloaders(batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched images shape: torch.Size([32, 1, 28, 28]) -> (batch_dim, color_channels, image_height, image_width)\n"
     ]
    }
   ],
   "source": [
    "# Get X and y from the first batch\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Batched images shape: {batch_X.shape} -> (batch_dim, color_channels, image_height, image_width)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module to process data embeddings\n",
    "data_embed_module = DataEmbeddings(in_channels=1,\n",
    "                                   patch_size=PATCH_SIZE,\n",
    "                                   num_patches=NUM_PATCHES,\n",
    "                                   embed_dims=EMBED_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data embeddings shape: torch.Size([32, 17, 48]) -> (batch_dim, num_patches + class_embedding, embedding_dims)\n"
     ]
    }
   ],
   "source": [
    "# Data embeddings and its shape\n",
    "data_embeddings = data_embed_module(batch_X)\n",
    "print(f\"Data embeddings shape: {data_embeddings.shape} -> (batch_dim, num_patches + class_embedding, embedding_dims)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer encoder module\n",
    "encoder_block = EncoderBlock(embed_dims=EMBED_DIMS,\n",
    "                             num_attn_heads=NUM_ATTENTION_HEADS,\n",
    "                             ratio_hidden_mlp=RATIO_HIDDEN_MLP,\n",
    "                             batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([32, 17, 48]) -> (batch_dim, num_patches + class_embedding, embedding_dims)\n"
     ]
    }
   ],
   "source": [
    "# Transformer encoder output\n",
    "encoder_output = encoder_block(data_embeddings)\n",
    "print(f\"Encoder output shape: {encoder_output.shape} -> (batch_dim, num_patches + class_embedding, embedding_dims)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the learnable embedding from the embeddings (include all batches and dimensions)\n",
    "classifier_input = encoder_output[:, 0, :]\n",
    "\n",
    "# Classifier which has number of classes as output\n",
    "classifier = torch.nn.Linear(in_features=EMBED_DIMS,\n",
    "                             out_features=len(class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([32, 10]) -> (batch_dim, num_classes)\n"
     ]
    }
   ],
   "source": [
    "# Output of ViT\n",
    "vit_out = classifier(classifier_input)\n",
    "print(f\"Model output shape: {vit_out.shape} -> (batch_dim, num_classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.0592, 0.1015, 0.0424, 0.0327, 0.0894, 0.1513, 0.0891, 0.0377, 0.1189,\n",
       "         0.2778],\n",
       "        [0.0601, 0.1020, 0.0424, 0.0324, 0.0903, 0.1511, 0.0880, 0.0378, 0.1146,\n",
       "         0.2813],\n",
       "        [0.0593, 0.1019, 0.0430, 0.0324, 0.0898, 0.1496, 0.0887, 0.0369, 0.1181,\n",
       "         0.2803],\n",
       "        [0.0559, 0.1046, 0.0444, 0.0341, 0.0894, 0.1437, 0.0910, 0.0410, 0.1225,\n",
       "         0.2733],\n",
       "        [0.0574, 0.1012, 0.0427, 0.0325, 0.0902, 0.1514, 0.0887, 0.0367, 0.1166,\n",
       "         0.2825],\n",
       "        [0.0575, 0.1026, 0.0420, 0.0337, 0.0900, 0.1501, 0.0891, 0.0358, 0.1181,\n",
       "         0.2812],\n",
       "        [0.0596, 0.1006, 0.0423, 0.0326, 0.0900, 0.1536, 0.0891, 0.0365, 0.1141,\n",
       "         0.2816],\n",
       "        [0.0578, 0.1030, 0.0424, 0.0332, 0.0907, 0.1479, 0.0904, 0.0388, 0.1209,\n",
       "         0.2749],\n",
       "        [0.0592, 0.1018, 0.0420, 0.0329, 0.0902, 0.1510, 0.0883, 0.0372, 0.1191,\n",
       "         0.2784],\n",
       "        [0.0601, 0.1005, 0.0420, 0.0317, 0.0896, 0.1532, 0.0870, 0.0359, 0.1169,\n",
       "         0.2831],\n",
       "        [0.0608, 0.1007, 0.0422, 0.0318, 0.0900, 0.1534, 0.0868, 0.0356, 0.1159,\n",
       "         0.2829],\n",
       "        [0.0584, 0.1018, 0.0427, 0.0328, 0.0890, 0.1509, 0.0896, 0.0372, 0.1177,\n",
       "         0.2797],\n",
       "        [0.0594, 0.1021, 0.0424, 0.0325, 0.0908, 0.1493, 0.0887, 0.0375, 0.1184,\n",
       "         0.2789],\n",
       "        [0.0585, 0.0999, 0.0420, 0.0323, 0.0908, 0.1529, 0.0885, 0.0356, 0.1165,\n",
       "         0.2828],\n",
       "        [0.0557, 0.1036, 0.0440, 0.0335, 0.0897, 0.1462, 0.0909, 0.0401, 0.1221,\n",
       "         0.2743],\n",
       "        [0.0554, 0.1035, 0.0431, 0.0335, 0.0900, 0.1476, 0.0908, 0.0380, 0.1212,\n",
       "         0.2771],\n",
       "        [0.0566, 0.1023, 0.0432, 0.0333, 0.0906, 0.1495, 0.0901, 0.0370, 0.1168,\n",
       "         0.2808],\n",
       "        [0.0599, 0.1010, 0.0427, 0.0320, 0.0898, 0.1524, 0.0881, 0.0372, 0.1143,\n",
       "         0.2825],\n",
       "        [0.0597, 0.0994, 0.0432, 0.0318, 0.0884, 0.1530, 0.0886, 0.0368, 0.1157,\n",
       "         0.2834],\n",
       "        [0.0586, 0.1008, 0.0423, 0.0324, 0.0898, 0.1517, 0.0883, 0.0359, 0.1172,\n",
       "         0.2829],\n",
       "        [0.0601, 0.1007, 0.0419, 0.0323, 0.0903, 0.1521, 0.0881, 0.0361, 0.1171,\n",
       "         0.2814],\n",
       "        [0.0601, 0.1004, 0.0426, 0.0319, 0.0891, 0.1530, 0.0878, 0.0366, 0.1157,\n",
       "         0.2828],\n",
       "        [0.0582, 0.1013, 0.0429, 0.0331, 0.0888, 0.1522, 0.0897, 0.0375, 0.1158,\n",
       "         0.2806],\n",
       "        [0.0598, 0.1007, 0.0427, 0.0321, 0.0897, 0.1519, 0.0885, 0.0369, 0.1155,\n",
       "         0.2822],\n",
       "        [0.0595, 0.1006, 0.0426, 0.0322, 0.0896, 0.1529, 0.0881, 0.0368, 0.1162,\n",
       "         0.2816],\n",
       "        [0.0596, 0.1004, 0.0419, 0.0326, 0.0907, 0.1533, 0.0882, 0.0362, 0.1159,\n",
       "         0.2813],\n",
       "        [0.0577, 0.0996, 0.0423, 0.0320, 0.0898, 0.1527, 0.0873, 0.0361, 0.1162,\n",
       "         0.2864],\n",
       "        [0.0593, 0.1011, 0.0426, 0.0323, 0.0888, 0.1514, 0.0875, 0.0369, 0.1160,\n",
       "         0.2841],\n",
       "        [0.0588, 0.1002, 0.0422, 0.0321, 0.0906, 0.1527, 0.0881, 0.0354, 0.1169,\n",
       "         0.2830],\n",
       "        [0.0576, 0.1010, 0.0408, 0.0326, 0.0904, 0.1518, 0.0880, 0.0366, 0.1195,\n",
       "         0.2817],\n",
       "        [0.0600, 0.1027, 0.0422, 0.0327, 0.0910, 0.1513, 0.0871, 0.0375, 0.1143,\n",
       "         0.2812],\n",
       "        [0.0600, 0.1022, 0.0426, 0.0324, 0.0906, 0.1511, 0.0886, 0.0382, 0.1142,\n",
       "         0.2801]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating probabilities of each class for the first batch\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "prob_out = softmax(vit_out)\n",
    "prob_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9, 9,\n",
       "        9, 9, 9, 9, 9, 9, 9, 9])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted classes from the un-trained model\n",
    "pred_class_idx = torch.argmax(prob_out, dim=1)\n",
    "pred_class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([9, 1, 7, 2, 4, 1, 3, 8, 8, 7, 5, 7, 6, 2, 2, 2, 0, 1, 1, 2, 7, 1, 3, 1,\n",
       "        7, 9, 5, 1, 6, 9, 1, 1])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual classes idx\n",
    "batch_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".replicating-vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
