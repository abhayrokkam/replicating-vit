{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "super_directory = os.path.abspath('..')\n",
    "sys.path.append(super_directory)\n",
    "\n",
    "from data_setup import get_dataloaders\n",
    "from vit import DataEmbeddings, EncoderBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Patches\n",
    "PATCH_SIZE = (16, 16)\n",
    "NUM_PATCHES = int((224 / 16) ** 2)\n",
    "\n",
    "# Patches to Embeddings\n",
    "EMBED_DIMS = 768\n",
    "\n",
    "# Number of Attention heads\n",
    "NUM_ATTENTION_HEADS = 4\n",
    "\n",
    "# Number of hidden layers in MLP block\n",
    "RATIO_HIDDEN_MLP = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_path = Path('../data/desserts')\n",
    "train_path = data_path / 'train'\n",
    "test_path = data_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dataloaders\n",
    "train_dataloader, test_dataloader, class_labels = get_dataloaders(train_path=train_path,\n",
    "                                                                  test_path=test_path,\n",
    "                                                                  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched images shape: torch.Size([32, 3, 224, 224]) -> (batch_dim, color_channels, image_height, image_width)\n"
     ]
    }
   ],
   "source": [
    "# Get X and y from the first batch\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Batched images shape: {batch_X.shape} -> (batch_dim, color_channels, image_height, image_width)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module to process data embeddings\n",
    "data_embed_module = DataEmbeddings(in_channels=3,\n",
    "                                   patch_size=PATCH_SIZE,\n",
    "                                   num_patches=NUM_PATCHES,\n",
    "                                   embed_dims=EMBED_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data embeddings shape: torch.Size([32, 197, 768]) -> (batch_dim, num_patches + class_embedding, embedding_dims)\n"
     ]
    }
   ],
   "source": [
    "# Data embeddings and its shape\n",
    "data_embeddings = data_embed_module(batch_X)\n",
    "print(f\"Data embeddings shape: {data_embeddings.shape} -> (batch_dim, num_patches + class_embedding, embedding_dims)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transformer encoder module\n",
    "encoder_block = EncoderBlock(embed_dims=EMBED_DIMS,\n",
    "                             num_attn_heads=NUM_ATTENTION_HEADS,\n",
    "                             ratio_hidden_mlp=RATIO_HIDDEN_MLP,\n",
    "                             batch_first=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([32, 197, 768]) -> (batch_dim, num_patches + class_embedding, embedding_dims)\n"
     ]
    }
   ],
   "source": [
    "# Transformer encoder output\n",
    "encoder_output = encoder_block(data_embeddings)\n",
    "print(f\"Encoder output shape: {encoder_output.shape} -> (batch_dim, num_patches + class_embedding, embedding_dims)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting the learnable embedding from the embeddings (include all batches and dimensions)\n",
    "classifier_input = encoder_output[:, 0, :]\n",
    "\n",
    "# Classifier which has number of classes as output\n",
    "classifier = torch.nn.Linear(in_features=EMBED_DIMS,\n",
    "                             out_features=len(class_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model output shape: torch.Size([32, 5]) -> (batch_dim, num_classes)\n"
     ]
    }
   ],
   "source": [
    "# Output of ViT\n",
    "vit_out = classifier(classifier_input)\n",
    "print(f\"Model output shape: {vit_out.shape} -> (batch_dim, num_classes)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.2159, 0.3264, 0.0294, 0.1021, 0.3262],\n",
       "        [0.2192, 0.3253, 0.0287, 0.1010, 0.3257],\n",
       "        [0.2189, 0.3253, 0.0286, 0.1011, 0.3262],\n",
       "        [0.2295, 0.3234, 0.0277, 0.0982, 0.3211],\n",
       "        [0.2206, 0.3250, 0.0286, 0.1002, 0.3256],\n",
       "        [0.2231, 0.3260, 0.0284, 0.0995, 0.3230],\n",
       "        [0.2176, 0.3256, 0.0289, 0.1014, 0.3264],\n",
       "        [0.2134, 0.3275, 0.0297, 0.1021, 0.3274],\n",
       "        [0.2163, 0.3265, 0.0289, 0.1015, 0.3268],\n",
       "        [0.2137, 0.3264, 0.0295, 0.1022, 0.3283],\n",
       "        [0.2222, 0.3260, 0.0280, 0.0998, 0.3241],\n",
       "        [0.2143, 0.3262, 0.0291, 0.1024, 0.3280],\n",
       "        [0.2201, 0.3248, 0.0283, 0.1008, 0.3261],\n",
       "        [0.2120, 0.3274, 0.0297, 0.1024, 0.3285],\n",
       "        [0.2122, 0.3267, 0.0297, 0.1027, 0.3287],\n",
       "        [0.2217, 0.3255, 0.0279, 0.0999, 0.3250],\n",
       "        [0.2094, 0.3270, 0.0299, 0.1035, 0.3302],\n",
       "        [0.2219, 0.3240, 0.0283, 0.1000, 0.3258],\n",
       "        [0.2256, 0.3239, 0.0278, 0.0994, 0.3232],\n",
       "        [0.2251, 0.3246, 0.0281, 0.0990, 0.3232],\n",
       "        [0.2128, 0.3264, 0.0297, 0.1026, 0.3285],\n",
       "        [0.2218, 0.3264, 0.0288, 0.0995, 0.3235],\n",
       "        [0.2169, 0.3271, 0.0294, 0.1011, 0.3255],\n",
       "        [0.2191, 0.3257, 0.0288, 0.1009, 0.3254],\n",
       "        [0.2263, 0.3255, 0.0280, 0.0987, 0.3215],\n",
       "        [0.2174, 0.3268, 0.0288, 0.1008, 0.3261],\n",
       "        [0.2163, 0.3262, 0.0292, 0.1017, 0.3266],\n",
       "        [0.2260, 0.3235, 0.0282, 0.0989, 0.3234],\n",
       "        [0.2202, 0.3267, 0.0285, 0.1001, 0.3244],\n",
       "        [0.2217, 0.3257, 0.0287, 0.1001, 0.3238],\n",
       "        [0.2131, 0.3249, 0.0296, 0.1029, 0.3295],\n",
       "        [0.2229, 0.3253, 0.0279, 0.0999, 0.3239]], grad_fn=<SoftmaxBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculating probabilities of each class for the first batch\n",
    "softmax = torch.nn.Softmax(dim=1)\n",
    "prob_out = softmax(vit_out)\n",
    "prob_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 4, 4, 1, 4, 1, 4, 1, 4, 4, 1, 4, 4, 4, 4, 1, 4, 4, 1, 1, 4, 1, 1, 1,\n",
       "        1, 1, 4, 1, 1, 1, 4, 1])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Predicted classes from the un-trained model\n",
    "pred_class_idx = torch.argmax(prob_out, dim=1)\n",
    "pred_class_idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 3, 0, 0, 0, 1, 0, 1, 1, 0, 4, 3, 2, 4, 3, 2, 1, 3, 1, 3, 3, 4, 1, 1,\n",
       "        2, 0, 1, 2, 0, 1, 0, 2])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Actual classes idx\n",
    "batch_y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".replicating-vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
