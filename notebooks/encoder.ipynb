{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "import torch\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "super_directory = os.path.abspath('..')\n",
    "sys.path.append(super_directory)\n",
    "\n",
    "from data_setup import get_dataloaders\n",
    "from vit import DataEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device agnostic code\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Patches\n",
    "PATCH_SIZE = (16, 16)\n",
    "NUM_PATCHES = int((224 / 16) ** 2)\n",
    "\n",
    "# Patches to Embeddings\n",
    "EMBED_DIMS = 768\n",
    "\n",
    "# Number of Attention heads\n",
    "NUM_ATTENTION_HEADS = 4\n",
    "\n",
    "# Number of hidden layers in MLP block\n",
    "RATIO_HIDDEN_MLP = 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data paths\n",
    "data_path = Path('../data/desserts')\n",
    "train_path = data_path / 'train'\n",
    "test_path = data_path / 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Getting the dataloaders\n",
    "train_dataloader, test_dataloader, class_labels = get_dataloaders(train_path=train_path,\n",
    "                                                                  test_path=test_path,\n",
    "                                                                  batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batched images shape: torch.Size([32, 3, 224, 224]) -> (batch_dim, color_channels, image_height, image_width)\n"
     ]
    }
   ],
   "source": [
    "# Get X and y from the first batch\n",
    "batch_X, batch_y = next(iter(train_dataloader))\n",
    "\n",
    "print(f\"Batched images shape: {batch_X.shape} -> (batch_dim, color_channels, image_height, image_width)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Module to process data embeddings\n",
    "data_embed_module = DataEmbeddings(in_channels=3,\n",
    "                                   patch_size=PATCH_SIZE,\n",
    "                                   num_patches=NUM_PATCHES,\n",
    "                                   embed_dims=EMBED_DIMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data embeddings shape: torch.Size([32, 197, 768]) -> (batch_dim, num_patches + class_embedding, embedding_dims)\n"
     ]
    }
   ],
   "source": [
    "# Data embeddings and its shape\n",
    "data_embeddings = data_embed_module(batch_X)\n",
    "print(f\"Data embeddings shape: {data_embeddings.shape} -> (batch_dim, num_patches + class_embedding, embedding_dims)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encoder Blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder blocks\n",
    "layer_norm = torch.nn.LayerNorm(normalized_shape=EMBED_DIMS)\n",
    "\n",
    "multi_head_attention = torch.nn.MultiheadAttention(embed_dim=EMBED_DIMS,\n",
    "                                                   num_heads=NUM_ATTENTION_HEADS,\n",
    "                                                   batch_first=True)\n",
    "\n",
    "multi_layer_perceptron = torch.nn.Sequential(\n",
    "    torch.nn.Linear(in_features=EMBED_DIMS,\n",
    "                    out_features=int(EMBED_DIMS * RATIO_HIDDEN_MLP)),\n",
    "    torch.nn.GELU(),\n",
    "    torch.nn.Linear(in_features=int(EMBED_DIMS * RATIO_HIDDEN_MLP),\n",
    "                    out_features=EMBED_DIMS))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-head attention block\n",
    "norm_out = layer_norm(data_embeddings)\n",
    "attn_output, attn_output_weights = multi_head_attention(query=norm_out, \n",
    "                                                        key=norm_out, \n",
    "                                                        value=norm_out)\n",
    "residual_out = attn_output + data_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-layer perceptron\n",
    "norm_out = layer_norm(residual_out)\n",
    "mlp_out = multi_layer_perceptron(norm_out)\n",
    "enc_out = mlp_out + residual_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder output shape: torch.Size([32, 197, 768]) -> (batch_dim, num_patches + class_embedding, embedding_dims)\n"
     ]
    }
   ],
   "source": [
    "# Output of Encoder\n",
    "print(f\"Encoder output shape: {enc_out.shape} -> (batch_dim, num_patches + class_embedding, embedding_dims)\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".replicating-vit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
